{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import unidecode\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import yaml\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from collections import defaultdict, Counter\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec, Doc2Vec, FastText\n",
    "from sklearn import metrics\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = \"\"\"\n",
    "class ExperimentConfigBuilder(ExperimentConfigBuilderBase):\n",
    "\n",
    "    default_config = dict(\n",
    "        test=False,\n",
    "        device=0,\n",
    "        maxlen=72,\n",
    "        vocab_mincount=5,\n",
    "        scale_batchsize=[],\n",
    "        validate_from=4,\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def modules(self):\n",
    "        return [\n",
    "            TextNormalizer,\n",
    "            TextTokenizer,\n",
    "            WordEmbeddingFeaturizer,\n",
    "            WordExtraFeaturizer,\n",
    "            SentenceExtraFeaturizer,\n",
    "            Embedding,\n",
    "            Encoder,\n",
    "            Aggregator,\n",
    "            MLP,\n",
    "        ]\n",
    "\n",
    "\n",
    "def build_model(config, embedding_matrix, n_sentence_extra_features):\n",
    "    embedding = Embedding(config, embedding_matrix)\n",
    "    encoder = Encoder(config, embedding.out_size)\n",
    "    aggregator = Aggregator(config)\n",
    "    mlp = MLP(config, encoder.out_size + n_sentence_extra_features)\n",
    "    out = nn.Linear(config.mlp_n_hiddens[-1], 1)\n",
    "    lossfunc = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    return BinaryClassifier(\n",
    "        embedding=embedding,\n",
    "        encoder=encoder,\n",
    "        aggregator=aggregator,\n",
    "        mlp=mlp,\n",
    "        out=out,\n",
    "        lossfunc=lossfunc,\n",
    "    )\n",
    "\n",
    "\n",
    "# =======  Preprocessing modules  =======\n",
    "\n",
    "class TextNormalizer(TextNormalizerPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TextTokenizer(TextTokenizerPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class WordEmbeddingFeaturizer(WordEmbeddingFeaturizerPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class WordExtraFeaturizer(WordExtraFeaturizerPresets):\n",
    "\n",
    "    default_config = dict(\n",
    "        word_extra_features=['idf', 'unk'],\n",
    "    )\n",
    "\n",
    "\n",
    "class SentenceExtraFeaturizer(SentenceExtraFeaturizerPresets):\n",
    "\n",
    "    default_config = dict(\n",
    "        sentence_extra_features=['char', 'word'],\n",
    "    )\n",
    "\n",
    "\n",
    "class Preprocessor(PreprocessorPresets):\n",
    "\n",
    "    embedding_sampling = 400\n",
    "\n",
    "    def build_word_features(self, word_embedding_featurizer,\n",
    "                            embedding_matrices, word_extra_features):\n",
    "        embedding = np.stack(list(embedding_matrices.values()))\n",
    "\n",
    "        # Concat embedding\n",
    "        embedding = np.concatenate(embedding, axis=1)\n",
    "        vocab = word_embedding_featurizer.vocab\n",
    "        embedding[vocab.lfq & vocab.unk] = 0\n",
    "\n",
    "        # Embedding random sampling\n",
    "        n_embed = embedding.shape[1]\n",
    "        n_select = self.embedding_sampling\n",
    "        idx = np.random.permutation(n_embed)[:n_select]\n",
    "        embedding = embedding[:, idx]\n",
    "\n",
    "        word_features = np.concatenate(\n",
    "            [embedding, word_extra_features], axis=1)\n",
    "        return word_features\n",
    "\n",
    "\n",
    "# =======  Training modules  =======\n",
    "\n",
    "class Embedding(EmbeddingPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Encoder(EncoderPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Aggregator(AggregatorPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MLP(MLPPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Ensembler(EnsemblerPresets):\n",
    "    pass\n",
    "\n",
    "\"\"\"\n",
    "os.environ['DATADIR'] = '/kaggle/input'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
