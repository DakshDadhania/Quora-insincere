{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.utils import shuffle\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "embedding_glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "embedding_fasttext = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "embedding_para = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "embedding_w2v = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "train_path = '../input/train.csv'\n",
    "test_path = '../input/test.csv'\n",
    "\n",
    "mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "                \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\",\n",
    "                \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"I will have\", \"i'm\": \"i am\",\n",
    "                \"i've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "                \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "                \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "                \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                \"so've\": \"so have\", \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n",
    "                \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "                \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
    "                \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "                \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "                \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
    "                \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "                \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "                \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
    "                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "                \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color',\n",
    "                'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n",
    "                'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n",
    "                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize',\n",
    "                'youtu ': 'youtube ', 'qoura': 'quora', 'sallary': 'salary', 'whta': 'what',\n",
    "                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
    "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doi': 'do I',\n",
    "                'thebest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation',\n",
    "                'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis',\n",
    "                'etherium': 'ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017',\n",
    "                '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess',\n",
    "                \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "                'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "puncts = '\\'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\",\n",
    "                 \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '”': '\"', '“': '\"', \"£\": \"e\",\n",
    "                 '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta',\n",
    "                 '∅': '', '³': '3', 'π': 'pi', '\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "for p in puncts:\n",
    "    punct_mapping[p] = ' %s ' % p\n",
    "\n",
    "p = re.compile('(\\[ math \\]).+(\\[ / math \\])')\n",
    "p_space = re.compile(r'[^\\x20-\\x7e]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  seeding functions\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed + 1)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed + 2)\n",
    "    random.seed(seed + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading & pre-processing\n",
    "\n",
    "def clean_text(text):\n",
    "    # clean latex maths\n",
    "    text = p.sub(' [ math ] ', text)\n",
    "    # clean invisible chars\n",
    "    text = p_space.sub(r'', text)\n",
    "    # clean punctuations\n",
    "    for punct in punct_mapping:\n",
    "        if punct in text:\n",
    "            text = text.replace(punct, punct_mapping[punct])\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        # replace contractions & correct misspells\n",
    "        token = mispell_dict.get(token.lower(), token)\n",
    "        tokens.append(token)\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "def load_data(train_path=train_path, test_path=test_path, debug=False):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    if debug:\n",
    "        train_df = train_df[:10000]\n",
    "        test_df = test_df[:10000]\n",
    "    s = time.time()\n",
    "    train_df['question_text'] = train_df['question_text'].apply(clean_text)\n",
    "    test_df['question_text'] = test_df['question_text'].apply(clean_text)\n",
    "    print('preprocssing {}s'.format(time.time() - s))\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary functions\n",
    "def build_counter(sents, splited=False):\n",
    "    counter = Counter()\n",
    "    for sent in tqdm(sents, ascii=True, desc='building conuter'):\n",
    "        if splited:\n",
    "            counter.update(sent)\n",
    "        else:\n",
    "            counter.update(sent.split())\n",
    "    return counter\n",
    "\n",
    "\n",
    "def build_vocab(counter, max_vocab_size):\n",
    "    vocab = {'token2id': {'<PAD>': 0, '<UNK>': max_vocab_size + 1}}\n",
    "    vocab['token2id'].update(\n",
    "        {token: _id + 1 for _id, (token, count) in\n",
    "         tqdm(enumerate(counter.most_common(max_vocab_size)), desc='building vocab')})\n",
    "    vocab['id2token'] = {v: k for k, v in vocab['token2id'].items()}\n",
    "    return vocab\n",
    "\n",
    "def tokens2ids(tokens, token2id):\n",
    "    seq = []\n",
    "    for token in tokens:\n",
    "        token_id = token2id.get(token, len(token2id) - 1)\n",
    "        seq.append(token_id)\n",
    "    return seq\n",
    "\n",
    "#  data set\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, vocab=None, num_max=None, max_seq_len=100,\n",
    "                 max_vocab_size=95000):\n",
    "        if num_max is not None:\n",
    "            df = df[:num_max]\n",
    "\n",
    "        self.src_sents = df['question_text'].tolist()\n",
    "        self.qids = df['qid'].values\n",
    "        if vocab is None:\n",
    "            src_counter = build_counter(self.src_sents)\n",
    "            vocab = build_vocab(src_counter, max_vocab_size)\n",
    "        self.vocab = vocab\n",
    "        if 'src_seqs' not in df.columns:\n",
    "            self.src_seqs = []\n",
    "            for sent in tqdm(self.src_sents, desc='tokenize'):\n",
    "                seq = tokens2ids(sent.split()[:max_seq_len], vocab['token2id'])\n",
    "                self.src_seqs.append(seq)\n",
    "        else:\n",
    "            self.src_seqs = df['src_seqs'].tolist()\n",
    "        if 'target' in df.columns:\n",
    "            self.targets = df['target'].values\n",
    "        else:\n",
    "            self.targets = np.random.randint(2, size=(len(self.src_sents),))\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sents)\n",
    "\n",
    "    # for bucket iterator\n",
    "    def get_keys(self):\n",
    "        lens = np.fromiter(\n",
    "            tqdm(((min(self.max_seq_len, len(c.split()))) for c in self.src_sents), desc='generate lens'),\n",
    "            dtype=np.int32)\n",
    "        return lens\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.qids[index], self.src_sents[index], self.src_seqs[index], self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dynamic padding\n",
    "def _pad_sequences(seqs):\n",
    "    lens = [len(seq) for seq in seqs]\n",
    "    max_len = max(lens)\n",
    "\n",
    "    padded_seqs = torch.zeros(len(seqs), max_len).long()\n",
    "    for i, seq in enumerate(seqs):\n",
    "        end = lens[i]\n",
    "        padded_seqs[i, :end] = torch.LongTensor(seq)\n",
    "    return padded_seqs, lens\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    qids, src_sents, src_seqs, targets, = zip(*data)\n",
    "    src_seqs, src_lens = _pad_sequences(src_seqs)\n",
    "    return qids, src_sents, src_seqs, src_lens, torch.FloatTensor(targets)\n",
    "\n",
    "\n",
    "#  bucket iterator\n",
    "def divide_chunks(l, n):\n",
    "    if n == len(l):\n",
    "        yield np.arange(len(l), dtype=np.int32), l\n",
    "    else:\n",
    "        # looping till length l\n",
    "        for i in range(0, len(l), n):\n",
    "            data = l[i:i + n]\n",
    "            yield np.arange(i, i + len(data), dtype=np.int32), data\n",
    "\n",
    "\n",
    "def prepare_buckets(lens, bucket_size, batch_size, shuffle_data=True, indices=None):\n",
    "    lens = -lens\n",
    "    assert bucket_size % batch_size == 0 or bucket_size == len(lens)\n",
    "    if indices is None:\n",
    "        if shuffle_data:\n",
    "            indices = shuffle(np.arange(len(lens), dtype=np.int32))\n",
    "            lens = lens[indices]\n",
    "        else:\n",
    "            indices = np.arange(len(lens), dtype=np.int32)\n",
    "    new_indices = []\n",
    "    extra_batch = None\n",
    "    for chunk_index, chunk in (divide_chunks(lens, bucket_size)):\n",
    "        # sort indices in bucket by descending order of length\n",
    "        indices_sorted = chunk_index[np.argsort(chunk, axis=-1)]\n",
    "        batches = []\n",
    "        for _, batch in divide_chunks(indices_sorted, batch_size):\n",
    "            if len(batch) == batch_size:\n",
    "                batches.append(batch.tolist())\n",
    "            else:\n",
    "                assert extra_batch is None\n",
    "                assert batch is not None\n",
    "                extra_batch = batch\n",
    "        # shuffling batches within buckets\n",
    "        if shuffle_data:\n",
    "            batches = shuffle(batches)\n",
    "        for batch in batches:\n",
    "            new_indices.extend(batch)\n",
    "\n",
    "    if extra_batch is not None:\n",
    "        new_indices.extend(extra_batch)\n",
    "    return indices[new_indices]\n",
    "\n",
    "\n",
    "class BucketSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, sort_keys, bucket_size=None, batch_size=1536, shuffle_data=True):\n",
    "        super().__init__(data_source)\n",
    "        self.shuffle = shuffle_data\n",
    "        self.batch_size = batch_size\n",
    "        self.sort_keys = sort_keys\n",
    "        self.bucket_size = bucket_size if bucket_size is not None else len(sort_keys)\n",
    "        if not shuffle_data:\n",
    "            self.index = prepare_buckets(self.sort_keys, bucket_size=self.bucket_size, batch_size=self.batch_size,\n",
    "                                         shuffle_data=self.shuffle)\n",
    "        else:\n",
    "            self.index = None\n",
    "        self.weights = None\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        assert w >= 0\n",
    "        total = np.sum(w)\n",
    "        if total != 1:\n",
    "            w = w / total\n",
    "        self.weights = w\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = None\n",
    "        if self.weights is not None:\n",
    "            total = len(self.sort_keys)\n",
    "\n",
    "            indices = np.random.choice(total, (total,), p=self.weights)\n",
    "        if self.shuffle:\n",
    "            self.index = prepare_buckets(self.sort_keys, bucket_size=self.bucket_size, batch_size=self.batch_size,\n",
    "                                         shuffle_data=self.shuffle, indices=indices)\n",
    "        return iter(self.index)\n",
    "\n",
    "    def get_reverse_indexes(self):\n",
    "        indexes = np.zeros((len(self.index),), dtype=np.int32)\n",
    "        for i, j in enumerate(self.index):\n",
    "            indexes[j] = i\n",
    "        return indexes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sort_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding stuffs\n",
    "def read_embedding(embedding_file):\n",
    "    \"\"\"\n",
    "    read embedding file into a dictionary\n",
    "    each line of the embedding file should in the format like  word 0.13 0.22 ... 0.44\n",
    "    :param embedding_file: path of the embedding.\n",
    "    :return: a dictionary of word to its embedding (numpy array)\n",
    "    \"\"\"\n",
    "    if os.path.basename(embedding_file) != 'wiki-news-300d-1M.vec':\n",
    "        skip_head = None\n",
    "    else:\n",
    "        skip_head = 0\n",
    "    if os.path.basename(embedding_file) == 'paragram_300_sl999.txt':\n",
    "        encoding = 'latin'\n",
    "    else:\n",
    "        encoding = 'utf-8'\n",
    "    embeddings_index = {}\n",
    "    t_chunks = pd.read_csv(embedding_file, index_col=0, skiprows=skip_head, encoding=encoding, sep=' ', header=None,\n",
    "                           quoting=3,\n",
    "                           doublequote=False, quotechar=None, engine='c', na_filter=False, low_memory=True,\n",
    "                           chunksize=10000)\n",
    "    for t in t_chunks:\n",
    "        for k, v in zip(t.index.values, t.values):\n",
    "            embeddings_index[k] = v.astype(np.float32)\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_emb(embedding_index, word, word_raw):\n",
    "    if word == word_raw:\n",
    "        return None\n",
    "    else:\n",
    "        return embedding_index.get(word, None)\n",
    "\n",
    "\n",
    "def embedding2numpy(embedding_path, word_index, num_words, embed_size, emb_mean=0., emb_std=0.5,\n",
    "                    report_stats=False):\n",
    "    embedding_index = read_embedding(embedding_path)\n",
    "    num_words = min(num_words + 2, len(word_index))\n",
    "    if report_stats:\n",
    "        all_coefs = []\n",
    "        for v in embedding_index.values():\n",
    "            all_coefs.append(v.reshape([-1, 1]))\n",
    "        all_coefs = np.concatenate(all_coefs)\n",
    "        print(all_coefs.mean(), all_coefs.std(), np.linalg.norm(all_coefs, axis=-1).mean())\n",
    "    embedding_matrix = np.zeros((num_words, embed_size), dtype=np.float32)\n",
    "    oov = 0\n",
    "    oov_cap = 0\n",
    "    oov_upper = 0\n",
    "    oov_lower = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i == 0:  # padding\n",
    "            continue\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word, None)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = get_emb(embedding_index, word.lower(), word)\n",
    "            if embedding_vector is None:\n",
    "                embedding_vector = get_emb(embedding_index, word.upper(), word)\n",
    "                if embedding_vector is None:\n",
    "                    embedding_vector = get_emb(embedding_index, word.capitalize(), word)\n",
    "                    if embedding_vector is None:\n",
    "                        oov += 1\n",
    "                        # embedding_vector = (np.zeros((1, embed_size)))\n",
    "                        embedding_vector = np.random.normal(emb_mean, emb_std, size=(1, embed_size))\n",
    "                    else:\n",
    "                        oov_lower += 1\n",
    "                else:\n",
    "                    oov_upper += 1\n",
    "            else:\n",
    "                oov_cap += 1\n",
    "\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    print('oov %d/%d/%d/%d/%d' % (oov, oov_cap, oov_upper, oov_lower, len(word_index)))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def load_embedding(vocab, max_vocab_size, embed_size):\n",
    "    # load embedding\n",
    "    embedding_matrix1 = embedding2numpy(embedding_glove, vocab['token2id'], max_vocab_size, embed_size,\n",
    "                                        emb_mean=-0.005838499, emb_std=0.48782197, report_stats=False)\n",
    "    # -0.005838499 0.48782197 0.37823704\n",
    "    # oov 9196\n",
    "    # embedding_matrix2 = embedding2numpy(embedding_fasttext, vocab.token2id, max_vocab_size, embed_size,\n",
    "    #                                    report_stats=False, emb_mean=-0.0033469985, emb_std=0.109855495, )\n",
    "    # -0.0033469985 0.109855495 0.07475414\n",
    "    # oov 12885\n",
    "    embedding_matrix2 = embedding2numpy(embedding_para, vocab['token2id'], max_vocab_size, embed_size,\n",
    "                                        emb_mean=-0.0053247833, emb_std=0.49346462, report_stats=False)\n",
    "    # -0.0053247833 0.49346462 0.3828983\n",
    "    # oov 9061\n",
    "    # embedding_w2v\n",
    "    # -0.003527845 0.13315111 0.09407869\n",
    "    # oov 18927\n",
    "    return [embedding_matrix1, embedding_matrix2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyclic learning rate\n",
    "def set_lr(optimizer, lr):\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "\n",
    "class CyclicLR:\n",
    "    def __init__(self, optimizer, base_lr=0.001, max_lr=0.002, step_size=300., mode='triangular',\n",
    "                 gamma=0.99994, scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "        self.optimizer = optimizer\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma ** x\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        if new_base_lr is not None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr is not None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size is not None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n",
    "                self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self):\n",
    "        if self.clr_iterations == 0:\n",
    "            set_lr(self.optimizer, self.base_lr)\n",
    "        else:\n",
    "            set_lr(self.optimizer, self.clr())\n",
    "\n",
    "    def on_batch_end(self):\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        set_lr(self.optimizer, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class Capsule(nn.Module):\n",
    "    def __init__(self, input_dim_capsule=1024, num_capsule=5, dim_capsule=5, routings=4):\n",
    "        super(Capsule, self).__init__()\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.activation = self.squash\n",
    "        self.W = nn.Parameter(\n",
    "            nn.init.xavier_normal_(torch.empty(1, input_dim_capsule, self.num_capsule * self.dim_capsule)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        u_hat_vecs = torch.matmul(x, self.W)\n",
    "        batch_size = x.size(0)\n",
    "        input_num_capsule = x.size(1)\n",
    "        u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule,\n",
    "                                      self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = u_hat_vecs.permute(0, 2, 1,\n",
    "                                        3).contiguous()  # (batch_size,num_capsule,input_num_capsule,dim_capsule)\n",
    "        with torch.no_grad():\n",
    "            b = torch.zeros_like(u_hat_vecs[:, :, :, 0])\n",
    "        for i in range(self.routings):\n",
    "            c = torch.nn.functional.softmax(b, dim=1)  # (batch_size,num_capsule,input_num_capsule)\n",
    "            outputs = self.activation(torch.sum(c.unsqueeze(-1) * u_hat_vecs, dim=2))  # bij,bijk->bik\n",
    "            if i < self.routings - 1:\n",
    "                b = (torch.sum(outputs.unsqueeze(2) * u_hat_vecs, dim=-1))  # bik,bijk->bij\n",
    "        return outputs  # (batch_size, num_capsule, dim_capsule)\n",
    "\n",
    "    def squash(self, x, axis=-1):\n",
    "        s_squared_norm = (x ** 2).sum(axis, keepdim=True)\n",
    "        scale = torch.sqrt(s_squared_norm + 1e-7)\n",
    "        return x / scale\n",
    "\n",
    "\n",
    "#  model\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, max_seq_len=70):\n",
    "        super().__init__()\n",
    "        self.attention_fc = nn.Linear(feature_dim, 1)\n",
    "        self.bias = nn.Parameter(torch.zeros(1, max_seq_len, 1, requires_grad=True))\n",
    "\n",
    "    def forward(self, rnn_output):\n",
    "        \"\"\"\n",
    "        forward attention scores and attended vectors\n",
    "        :param rnn_output: (#batch,#seq_len,#feature)\n",
    "        :return: attended_outputs (#batch,#feature)\n",
    "        \"\"\"\n",
    "        attention_weights = self.attention_fc(rnn_output)\n",
    "        seq_len = rnn_output.size(1)\n",
    "        attention_weights = self.bias[:, :seq_len, :] + attention_weights\n",
    "        attention_weights = torch.tanh(attention_weights)\n",
    "        attention_weights = torch.exp(attention_weights)\n",
    "        attention_weights_sum = torch.sum(attention_weights, dim=1, keepdim=True) + 1e-7\n",
    "        attention_weights = attention_weights / attention_weights_sum\n",
    "        attended = torch.sum(attention_weights * rnn_output, dim=1)\n",
    "        return attended\n",
    "\n",
    "\n",
    "class InsincereModel(nn.Module):\n",
    "    def __init__(self, device, hidden_dim, hidden_dim_fc, embedding_matrixs, vocab_size=None, embedding_dim=None,\n",
    "                 dropout=0.1, num_capsule=5, dim_capsule=5, capsule_out_dim=1, alpha=0.8, beta=0.8,\n",
    "                 finetuning_vocab_size=120002,\n",
    "                 embedding_mode='mixup', max_seq_len=70):\n",
    "        super(InsincereModel, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.embedding_mode = embedding_mode\n",
    "        self.finetuning_vocab_size = finetuning_vocab_size\n",
    "        self.alpha = alpha\n",
    "        vocab_size, embedding_dim = embedding_matrixs[0].shape\n",
    "        self.raw_embedding_weights = embedding_matrixs\n",
    "        self.embedding_0 = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).from_pretrained(\n",
    "            torch.from_numpy(embedding_matrixs[0]))\n",
    "        self.embedding_1 = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).from_pretrained(\n",
    "            torch.from_numpy(embedding_matrixs[1]))\n",
    "        self.embedding_mean = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).from_pretrained(\n",
    "            torch.from_numpy((embedding_matrixs[0] + embedding_matrixs[1]) / 2))\n",
    "        self.learnable_embedding = nn.Embedding(finetuning_vocab_size, embedding_dim, padding_idx=0)\n",
    "        nn.init.constant_(self.learnable_embedding.weight, 0)\n",
    "        self.learn_embedding = False\n",
    "        self.spatial_dropout = nn.Dropout2d(p=0.2)\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn0 = nn.LSTM(embedding_dim, int(hidden_dim / 2), num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.rnn1 = nn.GRU(hidden_dim, int(hidden_dim / 2), num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.capsule = Capsule(input_dim_capsule=self.hidden_dim, num_capsule=num_capsule, dim_capsule=dim_capsule)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.lincaps = nn.Linear(num_capsule * dim_capsule, capsule_out_dim)\n",
    "        self.attention1 = Attention(self.hidden_dim, max_seq_len=max_seq_len)\n",
    "        self.attention2 = Attention(self.hidden_dim, max_seq_len=max_seq_len)\n",
    "        self.fc = nn.Linear(hidden_dim * 4 + capsule_out_dim, hidden_dim_fc)\n",
    "        self.norm = torch.nn.LayerNorm(hidden_dim * 4 + capsule_out_dim)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout_linear = nn.Dropout(p=dropout)\n",
    "        self.hidden2out = nn.Linear(hidden_dim_fc, 1)\n",
    "\n",
    "    def set_embedding_mode(self, embedding_mode):\n",
    "        self.embedding_mode = embedding_mode\n",
    "\n",
    "    def enable_learning_embedding(self):\n",
    "        self.learn_embedding = True\n",
    "\n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def apply_spatial_dropout(self, emb):\n",
    "        emb = emb.permute(0, 2, 1).unsqueeze(-1)\n",
    "        emb = self.spatial_dropout(emb).squeeze(-1).permute(0, 2, 1)\n",
    "        return emb\n",
    "\n",
    "    def forward(self, seqs, lens, return_logits=True):\n",
    "        # forward embeddings\n",
    "        if self.embedding_mode == 'mixup':\n",
    "            emb0 = self.embedding_0(seqs)  # batch_size x seq_len x embedding_dim\n",
    "            emb1 = self.embedding_1(seqs)\n",
    "            prob = np.random.beta(self.alpha, self.beta, size=(seqs.size(0), 1, 1)).astype(np.float32)\n",
    "            prob = torch.from_numpy(prob).to(self.device)\n",
    "            emb = emb0 * prob + emb1 * (1 - prob)\n",
    "        elif self.embedding_mode == 'emb0':\n",
    "            emb = self.embedding_0(seqs)\n",
    "        elif self.embedding_mode == 'emb1':\n",
    "            emb = self.embedding_1(seqs)\n",
    "        elif self.embedding_mode == 'mean':\n",
    "            emb = self.embedding_mean(seqs)\n",
    "        else:\n",
    "            assert False\n",
    "        if self.learn_embedding:\n",
    "            seq_clamped = torch.clamp(seqs, 0, self.finetuning_vocab_size - 1)\n",
    "            emb_learned = self.learnable_embedding(seq_clamped)\n",
    "            emb = emb + emb_learned\n",
    "        emb = self.apply_spatial_dropout(emb)\n",
    "        # forward rnn encoder\n",
    "        lstm_output0, _ = self.rnn0(emb)\n",
    "        lstm_output1, _ = self.rnn1(lstm_output0)\n",
    "        # forward capsule\n",
    "        content3 = self.capsule(lstm_output1)\n",
    "        batch_size = content3.size(0)\n",
    "        content3 = content3.view(batch_size, -1)\n",
    "        content3 = self.dropout2(content3)\n",
    "        content3 = torch.relu(self.lincaps(content3))\n",
    "        # forward feature extractor\n",
    "        feature_att1 = self.attention1(lstm_output0)\n",
    "        feature_att2 = self.attention2(lstm_output1)\n",
    "        feature_avg2 = torch.mean(lstm_output1, dim=1)\n",
    "        feature_max2, _ = torch.max(lstm_output1, dim=1)\n",
    "        feature = torch.cat((feature_att1, feature_att2, feature_avg2, feature_max2, content3), dim=-1)\n",
    "        feature = self.norm(feature)\n",
    "        feature = self.dropout1(feature)\n",
    "        feature = torch.relu(feature)\n",
    "        # forward dense layer\n",
    "        out = self.fc(feature)\n",
    "        out = self.dropout_linear(out)\n",
    "        out = self.hidden2out(out)  # batch_size x 1\n",
    "        if not return_logits:\n",
    "            out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  util functions\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def margin_score(targets, predictions):\n",
    "    return ((targets == 1) * (1 - predictions) + (targets == 0) * (predictions)).mean()\n",
    "\n",
    "\n",
    "def report_perf(valid_dataset, predictions_va, threshold, idx, epoch_cur, desc='val set'):\n",
    "    val_f1 = f1_score(valid_dataset.targets, predictions_va > threshold)\n",
    "    val_auc = roc_auc_score(valid_dataset.targets, predictions_va)\n",
    "    val_margin = margin_score(valid_dataset.targets, predictions_va)\n",
    "    print('idx {} epoch {} {} f1 : {:.4f} auc : {:.4f} margin : {:.4f}'.format(\n",
    "        idx,\n",
    "        epoch_cur,\n",
    "        desc,\n",
    "        val_f1,\n",
    "        val_auc,\n",
    "        val_margin))\n",
    "\n",
    "\n",
    "def get_gpu_memory_usage(device_id):\n",
    "    return round(torch.cuda.max_memory_allocated(device_id) / 1000 / 1000)\n",
    "\n",
    "\n",
    "def avg(loss_list):\n",
    "    if len(loss_list) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sum(loss_list) / len(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def eval_model(model, data_iter, device, order_index=None):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in data_iter:\n",
    "            qid_batch, src_sents, src_seqs, src_lens, tgts = batch_data\n",
    "            src_seqs = src_seqs.to(device)\n",
    "            out = model(src_seqs, src_lens, return_logits=False)\n",
    "            predictions.append(out)\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    if order_index is not None:\n",
    "        predictions = predictions[order_index]\n",
    "    predictions = predictions.to('cpu').numpy().ravel()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "\n",
    "def cv(train_df, test_df, device=None, n_folds=10, shared_resources=None, share=True, **kwargs):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:{}\".format(0) if torch.cuda.is_available() else \"cpu\")\n",
    "    max_vocab_size = kwargs['max_vocab_size']\n",
    "    embed_size = kwargs['embed_size']\n",
    "    threshold = kwargs['threshold']\n",
    "    max_seq_len = kwargs['max_seq_len']\n",
    "    if shared_resources is None:\n",
    "        shared_resources = {}\n",
    "    if share:\n",
    "        if 'vocab' not in shared_resources:\n",
    "            # also include the test set\n",
    "\n",
    "            counter = build_counter(chain(train_df['question_text'], test_df['question_text']))\n",
    "            vocab = build_vocab(counter, max_vocab_size=max_vocab_size)\n",
    "            shared_resources['vocab'] = vocab\n",
    "            # tokenize sentences\n",
    "            seqs = []\n",
    "            for sent in tqdm(train_df['question_text'], desc='tokenize'):\n",
    "                seq = tokens2ids(sent.split()[:max_seq_len], vocab['token2id'])\n",
    "                seqs.append(seq)\n",
    "            train_df['src_seqs'] = seqs\n",
    "            seqs = []\n",
    "            for sent in tqdm(test_df['question_text'], desc='tokenize'):\n",
    "                seq = tokens2ids(sent.split()[:max_seq_len], vocab['token2id'])\n",
    "                seqs.append(seq)\n",
    "            test_df['src_seqs'] = seqs\n",
    "    if 'embedding_matrix' not in shared_resources:\n",
    "        embedding_matrix = load_embedding(shared_resources['vocab'], max_vocab_size, embed_size)\n",
    "        shared_resources['embedding_matrix'] = embedding_matrix\n",
    "    splits = list(\n",
    "        StratifiedKFold(n_splits=n_folds, shuffle=True).split(train_df['target'], train_df['target']))\n",
    "    scores = []\n",
    "    best_threshold = []\n",
    "    best_threshold_global = None\n",
    "    best_score = -1\n",
    "    predictions_train_reduced = []\n",
    "    targets_train = []\n",
    "    predictions_tes_reduced = np.zeros((len(test_df), n_folds))\n",
    "    predictions_te =  np.zeros((len(test_df),))\n",
    "    for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "        grow_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "        dev_df = train_df.iloc[valid_idx].reset_index(drop=True)\n",
    "        predictions_te_i, predictions_va, targets_va, best_threshold_i = main(grow_df, dev_df, test_df, device,\n",
    "                                                                              **kwargs,\n",
    "                                                                              idx=idx,\n",
    "                                                                              shared_resources=shared_resources,\n",
    "                                                                              return_reduced=True)\n",
    "        # predictions_va_raw shape (#len_va,n_models)\n",
    "        predictions_tes_reduced[:, idx] = predictions_te_i\n",
    "        scores.append([f1_score(targets_va, predictions_va > threshold), roc_auc_score(targets_va, predictions_va)])\n",
    "        best_threshold.append(best_threshold_i)\n",
    "        predictions_te += predictions_te_i / n_folds\n",
    "        predictions_train_reduced.append(predictions_va)\n",
    "        targets_train.append(targets_va)\n",
    "    # calculate model coefficient\n",
    "    coeff = (np.corrcoef(predictions_tes_reduced, rowvar=False).sum() - n_folds) / n_folds / (n_folds - 1)\n",
    "    # create data set for stacking\n",
    "    predictions_train_reduced = np.concatenate(predictions_train_reduced)\n",
    "    targets_train = np.concatenate(targets_train)  # len_train\n",
    "    # train optimal combining weights\n",
    "\n",
    "    # simple average\n",
    "    for t in np.arange(0, 1, 0.01):\n",
    "        score = f1_score(targets_train, predictions_train_reduced > t)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold_global = t\n",
    "    print('avg of best threshold {} macro-f1 best threshold {} best score {}'.format(best_threshold,\n",
    "                                                                                     best_threshold_global, best_score))\n",
    "    return predictions_te, predictions_te, scores, best_threshold_global, coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main routine\n",
    "def main(train_df, valid_df, test_df, device=None, epochs=3, fine_tuning_epochs=3, batch_size=512, learning_rate=0.001,\n",
    "         learning_rate_max_offset=0.001, dropout=0.1,\n",
    "         threshold=None,\n",
    "         max_vocab_size=95000, embed_size=300, max_seq_len=70, print_every_step=500, idx=0, shared_resources=None,\n",
    "         return_reduced=True):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:{}\".format(0) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if shared_resources is None:\n",
    "        shared_resources = {}\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    mean_len = AverageMeter()\n",
    "    # build vocab of raw df\n",
    "\n",
    "    if 'vocab' not in shared_resources:\n",
    "        counter = build_counter(chain(train_df['question_text'], test_df['question_text']))\n",
    "        vocab = build_vocab(counter, max_vocab_size=max_vocab_size)\n",
    "    else:\n",
    "        vocab = shared_resources['vocab']\n",
    "    if 'embedding_matrix' not in shared_resources:\n",
    "        embedding_matrix = load_embedding(vocab, max_vocab_size, embed_size)\n",
    "    else:\n",
    "        embedding_matrix = shared_resources['embedding_matrix']\n",
    "    # create test dataset\n",
    "    test_dataset = TextDataset(test_df, vocab=vocab, max_seq_len=max_seq_len)\n",
    "    tb = BucketSampler(test_dataset, test_dataset.get_keys(), batch_size=batch_size,\n",
    "                       shuffle_data=False)\n",
    "    test_iter = DataLoader(dataset=test_dataset,\n",
    "                           batch_size=batch_size,\n",
    "                           sampler=tb,\n",
    "                           # shuffle=False,\n",
    "                           num_workers=0,\n",
    "                           collate_fn=collate_fn)\n",
    "\n",
    "    train_dataset = TextDataset(train_df, vocab=vocab, max_seq_len=max_seq_len)\n",
    "    # keys = train_dataset.get_keys()  # for bucket sorting\n",
    "    valid_dataset = TextDataset(valid_df, vocab=vocab, max_seq_len=max_seq_len)\n",
    "    vb = BucketSampler(valid_dataset, valid_dataset.get_keys(), batch_size=batch_size,\n",
    "                       shuffle_data=False)\n",
    "    valid_index_reverse = vb.get_reverse_indexes()\n",
    "    # init model and optimizers\n",
    "    model = InsincereModel(device, hidden_dim=256, hidden_dim_fc=16, dropout=dropout,\n",
    "                           embedding_matrixs=embedding_matrix,\n",
    "                           vocab_size=len(vocab['token2id']),\n",
    "                           embedding_dim=embed_size, max_seq_len=max_seq_len)\n",
    "    if idx == 0:\n",
    "        print(model)\n",
    "        print('total trainable {}'.format(count_parameters(model)))\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=learning_rate)\n",
    "\n",
    "    # init iterator\n",
    "    train_iter = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            # shuffle=True,\n",
    "                            # sampler=NegativeSubSampler(train_dataset, train_dataset.targets),\n",
    "                            sampler=BucketSampler(train_dataset, train_dataset.get_keys(), bucket_size=batch_size * 20,\n",
    "                                                  batch_size=batch_size),\n",
    "                            num_workers=0,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "    valid_iter = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            sampler=vb,\n",
    "                            # shuffle=False,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "    # train model\n",
    "\n",
    "    loss_list = []\n",
    "    global_steps = 0\n",
    "    total_steps = epochs * len(train_iter)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    end = time.time()\n",
    "    predictions_tes = []\n",
    "    predictions_vas = []\n",
    "    n_fge = 0\n",
    "    clr = CyclicLR(optimizer, base_lr=learning_rate, max_lr=learning_rate + learning_rate_max_offset,\n",
    "                   step_size=300, mode='exp_range')\n",
    "    clr.on_train_begin()\n",
    "    fine_tuning_epochs = epochs - fine_tuning_epochs\n",
    "    predictions_te = None\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        fine_tuning = epoch >= fine_tuning_epochs\n",
    "        start_fine_tuning = fine_tuning_epochs == epoch\n",
    "        if start_fine_tuning:\n",
    "            model.enable_learning_embedding()\n",
    "            optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=learning_rate)\n",
    "            # fine tuning embedding layer\n",
    "            global_steps = 0\n",
    "            total_steps = (epochs - fine_tuning_epochs) * len(train_iter)\n",
    "            clr = CyclicLR(optimizer, base_lr=learning_rate, max_lr=learning_rate + learning_rate_max_offset,\n",
    "                           step_size=int(len(train_iter) / 8))\n",
    "            clr.on_train_begin()\n",
    "            predictions_te = np.zeros((len(test_df),))\n",
    "            predictions_va = np.zeros((len(valid_dataset.targets),))\n",
    "        for batch_data in train_iter:\n",
    "            data_time.update(time.time() - end)\n",
    "            qids, src_sents, src_seqs, src_lens, tgts = batch_data\n",
    "            mean_len.update(sum(src_lens))\n",
    "            src_seqs = src_seqs.to(device)\n",
    "            tgts = tgts.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(src_seqs, src_lens, return_logits=True).view(-1)\n",
    "            loss = loss_fn(out, tgts)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append(loss.detach().to('cpu').item())\n",
    "\n",
    "            global_steps += 1\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if global_steps % print_every_step == 0:\n",
    "                curr_gpu_memory_usage = get_gpu_memory_usage(device_id=torch.cuda.current_device())\n",
    "                print('Global step: {}/{} Total loss: {:.4f}  Current GPU memory '\n",
    "                      'usage: {} maxlen {} '.format(global_steps, total_steps, avg(loss_list), curr_gpu_memory_usage,\n",
    "                                                    mean_len.avg))\n",
    "                loss_list = []\n",
    "\n",
    "                # print(f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                #      f'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t')\n",
    "            if fine_tuning and global_steps % (2 * clr.step_size) == 0:\n",
    "                predictions_te_tmp2 = eval_model(model, test_iter, device)\n",
    "                predictions_va_tmp2 = eval_model(model, valid_iter, device, valid_index_reverse)\n",
    "                report_perf(valid_dataset, predictions_va_tmp2, threshold, idx, epoch,\n",
    "                            desc='val set mean')\n",
    "                predictions_te = predictions_te * n_fge + (\n",
    "                    predictions_te_tmp2)\n",
    "                predictions_va = predictions_va * n_fge + (\n",
    "                    predictions_va_tmp2)\n",
    "                predictions_te /= n_fge + 1\n",
    "                predictions_va /= n_fge + 1\n",
    "                report_perf(valid_dataset, predictions_va, threshold, idx, epoch\n",
    "                            , desc='val set (fge)')\n",
    "                predictions_tes.append(predictions_te_tmp2.reshape([-1, 1]))\n",
    "                predictions_vas.append(predictions_va_tmp2.reshape([-1, 1]))\n",
    "                n_fge += 1\n",
    "\n",
    "            clr.on_batch_end()\n",
    "        if not fine_tuning:\n",
    "            predictions_va = eval_model(model, valid_iter, device, valid_index_reverse)\n",
    "            report_perf(valid_dataset, predictions_va, threshold, idx, epoch)\n",
    "    # pprint(model.attention1.bias.data.to('cpu'))\n",
    "    # pprint(model.attention2.bias.data.to('cpu'))\n",
    "    # reorder index\n",
    "    if predictions_te is not None:\n",
    "        predictions_te = predictions_te[tb.get_reverse_indexes()]\n",
    "    else:\n",
    "        predictions_te = eval_model(model, test_iter, device, tb.get_reverse_indexes())\n",
    "    best_score = -1\n",
    "    best_threshold = None\n",
    "    for t in np.arange(0, 1, 0.01):\n",
    "        score = f1_score(valid_dataset.targets, predictions_va > t)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = t\n",
    "    print('best threshold on validation set: {:.2f} score {:.4f}'.format(best_threshold, best_score))\n",
    "    if not return_reduced and len(predictions_vas) > 0:\n",
    "        predictions_te = np.concatenate(predictions_tes, axis=1)\n",
    "        predictions_te = predictions_te[tb.get_reverse_indexes(), :]\n",
    "        predictions_va = np.concatenate(predictions_vas, axis=1)\n",
    "    # # Save the model\n",
    "    # model_path = 'trained_model.pth'\n",
    "    # save_model(model, model_path)\n",
    "    # print(\"Trained model saved to:\", model_path)\n",
    "\n",
    "    # # Make predictions\n",
    "    # predictions_te, _, _, _ = main(train_df, test_df, test_df, **args)\n",
    "\n",
    "    # # Calculate accuracy\n",
    "    # accuracy = calculate_accuracy(test_df['target'].values, predictions_te)\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # make predictions\n",
    "    return predictions_te, predictions_va, valid_dataset.targets, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0.dev20230722+cu121\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocssing 12.032110214233398s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building conuter: 1681928it [00:06, 261139.29it/s]\n",
      "building vocab: 120000it [00:00, 1344579.60it/s]\n",
      "tokenize: 100%|██████████| 1306122/1306122 [00:09<00:00, 141646.16it/s]\n",
      "tokenize: 100%|██████████| 375806/375806 [00:02<00:00, 158546.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov 6264/308/297/761/120002\n",
      "oov 6162/50165/0/0/120002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 375806it [00:00, 1047527.24it/s]\n",
      "c:\\Users\\daksh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n",
      "generate lens: 261225it [00:00, 873327.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InsincereModel(\n",
      "  (embedding_0): Embedding(120002, 300)\n",
      "  (embedding_1): Embedding(120002, 300)\n",
      "  (embedding_mean): Embedding(120002, 300)\n",
      "  (learnable_embedding): Embedding(120002, 300, padding_idx=0)\n",
      "  (spatial_dropout): Dropout2d(p=0.2, inplace=False)\n",
      "  (rnn0): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
      "  (rnn1): GRU(256, 128, batch_first=True, bidirectional=True)\n",
      "  (capsule): Capsule()\n",
      "  (dropout2): Dropout(p=0.3, inplace=False)\n",
      "  (lincaps): Linear(in_features=25, out_features=1, bias=True)\n",
      "  (attention1): Attention(\n",
      "    (attention_fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention2): Attention(\n",
      "    (attention_fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=1025, out_features=16, bias=True)\n",
      "  (norm): LayerNorm((1025,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (dropout_linear): Dropout(p=0.1, inplace=False)\n",
      "  (hidden2out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "total trainable 36762931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 1044897it [00:01, 861464.05it/s]\n",
      "c:\\Users\\daksh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 500/16328 Total loss: 0.1391  Current GPU memory usage: 1735 maxlen 7561.488 \n",
      "Global step: 1000/16328 Total loss: 0.1131  Current GPU memory usage: 1735 maxlen 7559.982 \n",
      "Global step: 1500/16328 Total loss: 0.1104  Current GPU memory usage: 1735 maxlen 7562.858666666667 \n",
      "Global step: 2000/16328 Total loss: 0.1080  Current GPU memory usage: 1735 maxlen 7558.7305 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [02:06<14:48, 126.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 0 val set f1 : 0.6689 auc : 0.9639 margin : 0.0598\n",
      "Global step: 2500/16328 Total loss: 0.1037  Current GPU memory usage: 1735 maxlen 7560.096 \n",
      "Global step: 3000/16328 Total loss: 0.1029  Current GPU memory usage: 1735 maxlen 7559.910333333333 \n",
      "Global step: 3500/16328 Total loss: 0.1015  Current GPU memory usage: 1735 maxlen 7559.757142857143 \n",
      "Global step: 4000/16328 Total loss: 0.1009  Current GPU memory usage: 1735 maxlen 7559.26375 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [04:03<12:03, 120.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 1 val set f1 : 0.6745 auc : 0.9674 margin : 0.0593\n",
      "Global step: 4500/16328 Total loss: 0.0956  Current GPU memory usage: 1735 maxlen 7555.108 \n",
      "Global step: 5000/16328 Total loss: 0.0966  Current GPU memory usage: 1735 maxlen 7558.5134 \n",
      "Global step: 5500/16328 Total loss: 0.0962  Current GPU memory usage: 1735 maxlen 7558.641636363636 \n",
      "Global step: 6000/16328 Total loss: 0.0968  Current GPU memory usage: 1735 maxlen 7559.131 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [05:58<09:51, 118.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 2 val set f1 : 0.6886 auc : 0.9698 margin : 0.0551\n",
      "Global step: 6500/16328 Total loss: 0.0913  Current GPU memory usage: 1735 maxlen 7557.107692307693 \n",
      "Global step: 7000/16328 Total loss: 0.0900  Current GPU memory usage: 1735 maxlen 7557.493 \n",
      "Global step: 7500/16328 Total loss: 0.0923  Current GPU memory usage: 1735 maxlen 7557.6736 \n",
      "Global step: 8000/16328 Total loss: 0.0927  Current GPU memory usage: 1735 maxlen 7557.80225 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [07:53<07:48, 117.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 3 val set f1 : 0.6874 auc : 0.9695 margin : 0.0592\n",
      "Global step: 8500/16328 Total loss: 0.0877  Current GPU memory usage: 1735 maxlen 7559.228588235294 \n",
      "Global step: 9000/16328 Total loss: 0.0867  Current GPU memory usage: 1735 maxlen 7557.801111111111 \n",
      "Global step: 9500/16328 Total loss: 0.0875  Current GPU memory usage: 1735 maxlen 7557.525368421053 \n",
      "Global step: 10000/16328 Total loss: 0.0874  Current GPU memory usage: 1735 maxlen 7558.337 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [09:49<05:49, 116.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 4 val set f1 : 0.6912 auc : 0.9702 margin : 0.0507\n",
      "Global step: 10500/16328 Total loss: 0.0836  Current GPU memory usage: 1735 maxlen 7558.371904761905 \n",
      "Global step: 11000/16328 Total loss: 0.0821  Current GPU memory usage: 1735 maxlen 7556.566181818182 \n",
      "Global step: 11500/16328 Total loss: 0.0843  Current GPU memory usage: 1735 maxlen 7559.022434782609 \n",
      "Global step: 12000/16328 Total loss: 0.0835  Current GPU memory usage: 1735 maxlen 7558.644083333334 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [11:44<03:52, 116.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 5 val set f1 : 0.6861 auc : 0.9693 margin : 0.0508\n",
      "Global step: 500/4082 Total loss: 0.0836  Current GPU memory usage: 2024 maxlen 7558.218107641613 \n",
      "idx 0 epoch 6 val set mean f1 : 0.6867 auc : 0.9685 margin : 0.0496\n",
      "idx 0 epoch 6 val set (fge) f1 : 0.6867 auc : 0.9685 margin : 0.0496\n",
      "Global step: 1000/4082 Total loss: 0.0831  Current GPU memory usage: 2024 maxlen 7558.353616186018 \n",
      "idx 0 epoch 6 val set mean f1 : 0.6885 auc : 0.9685 margin : 0.0539\n",
      "idx 0 epoch 6 val set (fge) f1 : 0.6942 auc : 0.9700 margin : 0.0517\n",
      "Global step: 1500/4082 Total loss: 0.0859  Current GPU memory usage: 2024 maxlen 7558.596318929143 \n",
      "idx 0 epoch 6 val set mean f1 : 0.6873 auc : 0.9693 margin : 0.0511\n",
      "idx 0 epoch 6 val set (fge) f1 : 0.6961 auc : 0.9709 margin : 0.0515\n",
      "Global step: 2000/4082 Total loss: 0.0862  Current GPU memory usage: 2024 maxlen 7558.420749684122 \n",
      "idx 0 epoch 6 val set mean f1 : 0.6930 auc : 0.9698 margin : 0.0509\n",
      "idx 0 epoch 6 val set (fge) f1 : 0.6998 auc : 0.9715 margin : 0.0514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [15:47<02:37, 157.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 2500/4082 Total loss: 0.0699  Current GPU memory usage: 2024 maxlen 7558.167842126679 \n",
      "idx 0 epoch 7 val set mean f1 : 0.6837 auc : 0.9672 margin : 0.0500\n",
      "idx 0 epoch 7 val set (fge) f1 : 0.7007 auc : 0.9717 margin : 0.0511\n",
      "Global step: 3000/4082 Total loss: 0.0708  Current GPU memory usage: 2024 maxlen 7558.195133149678 \n",
      "idx 0 epoch 7 val set mean f1 : 0.6812 auc : 0.9673 margin : 0.0516\n",
      "idx 0 epoch 7 val set (fge) f1 : 0.7015 auc : 0.9717 margin : 0.0512\n",
      "Global step: 3500/4082 Total loss: 0.0729  Current GPU memory usage: 2024 maxlen 7558.180236250476 \n",
      "idx 0 epoch 7 val set mean f1 : 0.6812 auc : 0.9671 margin : 0.0506\n",
      "idx 0 epoch 7 val set (fge) f1 : 0.7016 auc : 0.9717 margin : 0.0511\n",
      "Global step: 4000/4082 Total loss: 0.0752  Current GPU memory usage: 2024 maxlen 7558.4679305675245 \n",
      "idx 0 epoch 7 val set mean f1 : 0.6818 auc : 0.9676 margin : 0.0500\n",
      "idx 0 epoch 7 val set (fge) f1 : 0.7005 auc : 0.9718 margin : 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [19:49<00:00, 148.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold on validation set: 0.29 score 0.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 375806it [00:00, 1077460.25it/s]\n",
      "c:\\Users\\daksh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n",
      "generate lens: 261225it [00:00, 938752.95it/s]\n",
      "generate lens: 1044897it [00:01, 969572.85it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 500/16328 Total loss: 0.1400  Current GPU memory usage: 2024 maxlen 7570.062 \n",
      "Global step: 1000/16328 Total loss: 0.1153  Current GPU memory usage: 2024 maxlen 7555.91 \n",
      "Global step: 1500/16328 Total loss: 0.1092  Current GPU memory usage: 2024 maxlen 7553.864 \n",
      "Global step: 2000/16328 Total loss: 0.1069  Current GPU memory usage: 2024 maxlen 7554.3625 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [01:54<13:24, 114.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 0 val set f1 : 0.6607 auc : 0.9629 margin : 0.0647\n",
      "Global step: 2500/16328 Total loss: 0.1037  Current GPU memory usage: 2024 maxlen 7552.5432 \n",
      "Global step: 3000/16328 Total loss: 0.1020  Current GPU memory usage: 2024 maxlen 7555.788666666666 \n",
      "Global step: 3500/16328 Total loss: 0.1019  Current GPU memory usage: 2024 maxlen 7552.6197142857145 \n",
      "Global step: 4000/16328 Total loss: 0.1000  Current GPU memory usage: 2024 maxlen 7554.69325 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [03:49<11:29, 114.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 1 val set f1 : 0.6811 auc : 0.9672 margin : 0.0568\n",
      "Global step: 4500/16328 Total loss: 0.0958  Current GPU memory usage: 2024 maxlen 7554.091111111111 \n",
      "Global step: 5000/16328 Total loss: 0.0952  Current GPU memory usage: 2024 maxlen 7555.4988 \n",
      "Global step: 5500/16328 Total loss: 0.0962  Current GPU memory usage: 2024 maxlen 7553.9729090909095 \n",
      "Global step: 6000/16328 Total loss: 0.0949  Current GPU memory usage: 2024 maxlen 7555.149833333333 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [05:45<09:35, 115.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 2 val set f1 : 0.6873 auc : 0.9686 margin : 0.0549\n",
      "Global step: 6500/16328 Total loss: 0.0909  Current GPU memory usage: 2024 maxlen 7552.566769230769 \n",
      "Global step: 7000/16328 Total loss: 0.0912  Current GPU memory usage: 2024 maxlen 7554.703 \n",
      "Global step: 7500/16328 Total loss: 0.0913  Current GPU memory usage: 2024 maxlen 7554.7116 \n",
      "Global step: 8000/16328 Total loss: 0.0910  Current GPU memory usage: 2024 maxlen 7555.710875 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [07:40<07:40, 115.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 3 val set f1 : 0.6857 auc : 0.9690 margin : 0.0556\n",
      "Global step: 8500/16328 Total loss: 0.0865  Current GPU memory usage: 2024 maxlen 7553.9396470588235 \n",
      "Global step: 9000/16328 Total loss: 0.0868  Current GPU memory usage: 2024 maxlen 7554.9835555555555 \n",
      "Global step: 9500/16328 Total loss: 0.0872  Current GPU memory usage: 2024 maxlen 7552.796105263158 \n",
      "Global step: 10000/16328 Total loss: 0.0876  Current GPU memory usage: 2024 maxlen 7554.7513 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [09:35<05:45, 115.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 4 val set f1 : 0.6886 auc : 0.9696 margin : 0.0539\n",
      "Global step: 10500/16328 Total loss: 0.0824  Current GPU memory usage: 2024 maxlen 7551.9489523809525 \n",
      "Global step: 11000/16328 Total loss: 0.0824  Current GPU memory usage: 2024 maxlen 7553.819181818182 \n",
      "Global step: 11500/16328 Total loss: 0.0826  Current GPU memory usage: 2024 maxlen 7553.825739130435 \n",
      "Global step: 12000/16328 Total loss: 0.0838  Current GPU memory usage: 2024 maxlen 7554.24225 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [11:29<03:49, 114.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 5 val set f1 : 0.6831 auc : 0.9688 margin : 0.0556\n",
      "Global step: 500/4082 Total loss: 0.0817  Current GPU memory usage: 2024 maxlen 7554.310999529264 \n",
      "idx 1 epoch 6 val set mean f1 : 0.6841 auc : 0.9679 margin : 0.0521\n",
      "idx 1 epoch 6 val set (fge) f1 : 0.6841 auc : 0.9679 margin : 0.0521\n",
      "Global step: 1000/4082 Total loss: 0.0844  Current GPU memory usage: 2024 maxlen 7554.293371583874 \n",
      "idx 1 epoch 6 val set mean f1 : 0.6806 auc : 0.9684 margin : 0.0547\n",
      "idx 1 epoch 6 val set (fge) f1 : 0.6892 auc : 0.9697 margin : 0.0534\n",
      "Global step: 1500/4082 Total loss: 0.0853  Current GPU memory usage: 2024 maxlen 7553.960424850866 \n",
      "idx 1 epoch 6 val set mean f1 : 0.6839 auc : 0.9686 margin : 0.0518\n",
      "idx 1 epoch 6 val set (fge) f1 : 0.6933 auc : 0.9704 margin : 0.0529\n",
      "Global step: 2000/4082 Total loss: 0.0870  Current GPU memory usage: 2024 maxlen 7554.1293696476205 \n",
      "idx 1 epoch 6 val set mean f1 : 0.6893 auc : 0.9693 margin : 0.0506\n",
      "idx 1 epoch 6 val set (fge) f1 : 0.6961 auc : 0.9710 margin : 0.0523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [15:31<02:36, 156.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 2500/4082 Total loss: 0.0688  Current GPU memory usage: 2024 maxlen 7553.7648853926485 \n",
      "idx 1 epoch 7 val set mean f1 : 0.6795 auc : 0.9668 margin : 0.0497\n",
      "idx 1 epoch 7 val set (fge) f1 : 0.6971 auc : 0.9710 margin : 0.0518\n",
      "Global step: 3000/4082 Total loss: 0.0706  Current GPU memory usage: 2024 maxlen 7554.164370982553 \n",
      "idx 1 epoch 7 val set mean f1 : 0.6790 auc : 0.9673 margin : 0.0501\n",
      "idx 1 epoch 7 val set (fge) f1 : 0.6962 auc : 0.9711 margin : 0.0515\n",
      "Global step: 3500/4082 Total loss: 0.0717  Current GPU memory usage: 2024 maxlen 7553.862568271307 \n",
      "idx 1 epoch 7 val set mean f1 : 0.6796 auc : 0.9671 margin : 0.0488\n",
      "idx 1 epoch 7 val set (fge) f1 : 0.6968 auc : 0.9711 margin : 0.0511\n",
      "Global step: 4000/4082 Total loss: 0.0740  Current GPU memory usage: 2024 maxlen 7553.648036439739 \n",
      "idx 1 epoch 7 val set mean f1 : 0.6802 auc : 0.9667 margin : 0.0501\n",
      "idx 1 epoch 7 val set (fge) f1 : 0.6966 auc : 0.9712 margin : 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [19:33<00:00, 146.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold on validation set: 0.31 score 0.6966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 375806it [00:00, 1111130.81it/s]\n",
      "c:\\Users\\daksh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n",
      "generate lens: 261224it [00:00, 922607.65it/s]\n",
      "generate lens: 1044898it [00:01, 965544.68it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 500/16328 Total loss: 0.1380  Current GPU memory usage: 2024 maxlen 7563.98 \n",
      "Global step: 1000/16328 Total loss: 0.1155  Current GPU memory usage: 2024 maxlen 7559.483 \n",
      "Global step: 1500/16328 Total loss: 0.1090  Current GPU memory usage: 2024 maxlen 7554.685333333333 \n",
      "Global step: 2000/16328 Total loss: 0.1072  Current GPU memory usage: 2024 maxlen 7555.57 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [01:57<13:43, 117.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 0 val set f1 : 0.6553 auc : 0.9636 margin : 0.0714\n",
      "Global step: 2500/16328 Total loss: 0.1050  Current GPU memory usage: 2024 maxlen 7556.1948 \n",
      "Global step: 3000/16328 Total loss: 0.1023  Current GPU memory usage: 2024 maxlen 7556.0453333333335 \n",
      "Global step: 3500/16328 Total loss: 0.1007  Current GPU memory usage: 2024 maxlen 7556.563428571429 \n",
      "Global step: 4000/16328 Total loss: 0.0998  Current GPU memory usage: 2024 maxlen 7552.63525 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [05:17<16:34, 165.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 1 val set f1 : 0.6815 auc : 0.9678 margin : 0.0571\n",
      "Global step: 4500/16328 Total loss: 0.0956  Current GPU memory usage: 2024 maxlen 7553.695555555556 \n",
      "Global step: 5000/16328 Total loss: 0.0953  Current GPU memory usage: 2024 maxlen 7556.2428 \n",
      "Global step: 5500/16328 Total loss: 0.0970  Current GPU memory usage: 2024 maxlen 7556.235272727273 \n",
      "Global step: 6000/16328 Total loss: 0.0955  Current GPU memory usage: 2024 maxlen 7556.494 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [08:35<15:03, 180.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 2 val set f1 : 0.6896 auc : 0.9696 margin : 0.0529\n",
      "Global step: 6500/16328 Total loss: 0.0916  Current GPU memory usage: 2024 maxlen 7556.647384615385 \n",
      "Global step: 7000/16328 Total loss: 0.0905  Current GPU memory usage: 2024 maxlen 7556.623285714286 \n",
      "Global step: 7500/16328 Total loss: 0.0915  Current GPU memory usage: 2024 maxlen 7556.6252 \n",
      "Global step: 8000/16328 Total loss: 0.0914  Current GPU memory usage: 2024 maxlen 7557.058 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [11:53<12:30, 187.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 3 val set f1 : 0.6922 auc : 0.9698 margin : 0.0517\n",
      "Global step: 8500/16328 Total loss: 0.0872  Current GPU memory usage: 2024 maxlen 7555.6085882352945 \n",
      "Global step: 9000/16328 Total loss: 0.0871  Current GPU memory usage: 2024 maxlen 7556.728555555555 \n",
      "Global step: 9500/16328 Total loss: 0.0868  Current GPU memory usage: 2024 maxlen 7555.690315789474 \n",
      "Global step: 10000/16328 Total loss: 0.0867  Current GPU memory usage: 2024 maxlen 7556.4155 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [15:12<09:34, 191.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 4 val set f1 : 0.6942 auc : 0.9704 margin : 0.0541\n",
      "Global step: 10500/16328 Total loss: 0.0831  Current GPU memory usage: 2024 maxlen 7554.667142857143 \n",
      "Global step: 11000/16328 Total loss: 0.0819  Current GPU memory usage: 2024 maxlen 7555.7292727272725 \n",
      "Global step: 11500/16328 Total loss: 0.0824  Current GPU memory usage: 2024 maxlen 7555.761217391304 \n",
      "Global step: 12000/16328 Total loss: 0.0841  Current GPU memory usage: 2024 maxlen 7554.2981666666665 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [18:30<06:27, 193.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 5 val set f1 : 0.6918 auc : 0.9696 margin : 0.0522\n",
      "Global step: 500/4082 Total loss: 0.0817  Current GPU memory usage: 2024 maxlen 7555.063784716774 \n",
      "idx 2 epoch 6 val set mean f1 : 0.6862 auc : 0.9686 margin : 0.0551\n",
      "idx 2 epoch 6 val set (fge) f1 : 0.6862 auc : 0.9686 margin : 0.0551\n",
      "Global step: 1000/4082 Total loss: 0.0841  Current GPU memory usage: 2024 maxlen 7555.071342292013 \n",
      "idx 2 epoch 6 val set mean f1 : 0.6904 auc : 0.9694 margin : 0.0517\n",
      "idx 2 epoch 6 val set (fge) f1 : 0.6952 auc : 0.9705 margin : 0.0534\n",
      "Global step: 1500/4082 Total loss: 0.0861  Current GPU memory usage: 2024 maxlen 7555.7518550851155 \n",
      "idx 2 epoch 6 val set mean f1 : 0.6901 auc : 0.9700 margin : 0.0510\n",
      "idx 2 epoch 6 val set (fge) f1 : 0.6994 auc : 0.9714 margin : 0.0526\n",
      "Global step: 2000/4082 Total loss: 0.0873  Current GPU memory usage: 2024 maxlen 7555.538817913801 \n",
      "idx 2 epoch 6 val set mean f1 : 0.6924 auc : 0.9697 margin : 0.0506\n",
      "idx 2 epoch 6 val set (fge) f1 : 0.7016 auc : 0.9718 margin : 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [25:25<04:26, 266.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 2500/4082 Total loss: 0.0691  Current GPU memory usage: 2024 maxlen 7555.50501831005 \n",
      "idx 2 epoch 7 val set mean f1 : 0.6835 auc : 0.9669 margin : 0.0490\n",
      "idx 2 epoch 7 val set (fge) f1 : 0.7011 auc : 0.9718 margin : 0.0515\n",
      "Global step: 3000/4082 Total loss: 0.0706  Current GPU memory usage: 2024 maxlen 7555.640955004591 \n",
      "idx 2 epoch 7 val set mean f1 : 0.6824 auc : 0.9668 margin : 0.0510\n",
      "idx 2 epoch 7 val set (fge) f1 : 0.7018 auc : 0.9718 margin : 0.0514\n",
      "Global step: 3500/4082 Total loss: 0.0727  Current GPU memory usage: 2024 maxlen 7555.300076209831 \n",
      "idx 2 epoch 7 val set mean f1 : 0.6869 auc : 0.9682 margin : 0.0513\n",
      "idx 2 epoch 7 val set (fge) f1 : 0.7015 auc : 0.9719 margin : 0.0514\n",
      "Global step: 4000/4082 Total loss: 0.0760  Current GPU memory usage: 2024 maxlen 7555.317555090483 \n",
      "idx 2 epoch 7 val set mean f1 : 0.6879 auc : 0.9679 margin : 0.0505\n",
      "idx 2 epoch 7 val set (fge) f1 : 0.7018 auc : 0.9720 margin : 0.0513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [32:21<00:00, 242.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold on validation set: 0.34 score 0.7021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 375806it [00:00, 1065349.75it/s]\n",
      "c:\\Users\\daksh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n",
      "generate lens: 261224it [00:00, 986941.29it/s]\n",
      "generate lens: 1044898it [00:01, 1000400.80it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 500/16328 Total loss: 0.1383  Current GPU memory usage: 2024 maxlen 7553.494 \n",
      "Global step: 1000/16328 Total loss: 0.1148  Current GPU memory usage: 2024 maxlen 7549.947 \n",
      "Global step: 1500/16328 Total loss: 0.1091  Current GPU memory usage: 2024 maxlen 7552.907333333334 \n",
      "Global step: 2000/16328 Total loss: 0.1078  Current GPU memory usage: 2024 maxlen 7553.713 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [03:18<23:07, 198.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 0 val set f1 : 0.6603 auc : 0.9617 margin : 0.0571\n",
      "Global step: 2500/16328 Total loss: 0.1039  Current GPU memory usage: 2024 maxlen 7554.2844 \n",
      "Global step: 3000/16328 Total loss: 0.1011  Current GPU memory usage: 2024 maxlen 7551.841666666666 \n",
      "Global step: 3500/16328 Total loss: 0.1012  Current GPU memory usage: 2024 maxlen 7552.491714285714 \n",
      "Global step: 4000/16328 Total loss: 0.1001  Current GPU memory usage: 2024 maxlen 7552.5765 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [06:36<19:49, 198.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 1 val set f1 : 0.6743 auc : 0.9660 margin : 0.0586\n",
      "Global step: 4500/16328 Total loss: 0.0970  Current GPU memory usage: 2024 maxlen 7554.174666666667 \n",
      "Global step: 5000/16328 Total loss: 0.0946  Current GPU memory usage: 2024 maxlen 7553.951 \n",
      "Global step: 5500/16328 Total loss: 0.0943  Current GPU memory usage: 2024 maxlen 7553.288 \n",
      "Global step: 6000/16328 Total loss: 0.0970  Current GPU memory usage: 2024 maxlen 7552.526666666667 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [09:54<16:31, 198.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 2 val set f1 : 0.6752 auc : 0.9674 margin : 0.0574\n",
      "Global step: 6500/16328 Total loss: 0.0900  Current GPU memory usage: 2024 maxlen 7551.606615384615 \n",
      "Global step: 7000/16328 Total loss: 0.0913  Current GPU memory usage: 2024 maxlen 7552.415 \n",
      "Global step: 7500/16328 Total loss: 0.0900  Current GPU memory usage: 2024 maxlen 7551.210666666667 \n",
      "Global step: 8000/16328 Total loss: 0.0918  Current GPU memory usage: 2024 maxlen 7552.979375 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [13:13<13:13, 198.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 3 val set f1 : 0.6820 auc : 0.9681 margin : 0.0544\n",
      "Global step: 8500/16328 Total loss: 0.0868  Current GPU memory usage: 2024 maxlen 7551.794 \n",
      "Global step: 9000/16328 Total loss: 0.0861  Current GPU memory usage: 2024 maxlen 7551.571666666667 \n",
      "Global step: 9500/16328 Total loss: 0.0872  Current GPU memory usage: 2024 maxlen 7551.742736842105 \n",
      "Global step: 10000/16328 Total loss: 0.0862  Current GPU memory usage: 2024 maxlen 7550.9046 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [16:31<09:55, 198.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 4 val set f1 : 0.6836 auc : 0.9686 margin : 0.0534\n",
      "Global step: 10500/16328 Total loss: 0.0828  Current GPU memory usage: 2024 maxlen 7551.987904761905 \n",
      "Global step: 11000/16328 Total loss: 0.0827  Current GPU memory usage: 2024 maxlen 7552.528727272727 \n",
      "Global step: 11500/16328 Total loss: 0.0823  Current GPU memory usage: 2024 maxlen 7552.089565217391 \n",
      "Global step: 12000/16328 Total loss: 0.0830  Current GPU memory usage: 2024 maxlen 7551.485 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [19:50<06:36, 198.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 5 val set f1 : 0.6834 auc : 0.9683 margin : 0.0531\n",
      "Global step: 500/4082 Total loss: 0.0818  Current GPU memory usage: 2024 maxlen 7551.546838223757 \n",
      "idx 3 epoch 6 val set mean f1 : 0.6811 auc : 0.9668 margin : 0.0513\n",
      "idx 3 epoch 6 val set (fge) f1 : 0.6811 auc : 0.9668 margin : 0.0513\n",
      "Global step: 1000/4082 Total loss: 0.0836  Current GPU memory usage: 2024 maxlen 7551.764683678091 \n",
      "idx 3 epoch 6 val set mean f1 : 0.6819 auc : 0.9677 margin : 0.0524\n",
      "idx 3 epoch 6 val set (fge) f1 : 0.6891 auc : 0.9689 margin : 0.0519\n",
      "Global step: 1500/4082 Total loss: 0.0855  Current GPU memory usage: 2024 maxlen 7551.950967554198 \n",
      "idx 3 epoch 6 val set mean f1 : 0.6853 auc : 0.9682 margin : 0.0517\n",
      "idx 3 epoch 6 val set (fge) f1 : 0.6923 auc : 0.9697 margin : 0.0518\n",
      "Global step: 2000/4082 Total loss: 0.0865  Current GPU memory usage: 2024 maxlen 7552.051944405447 \n",
      "idx 3 epoch 6 val set mean f1 : 0.6843 auc : 0.9684 margin : 0.0494\n",
      "idx 3 epoch 6 val set (fge) f1 : 0.6933 auc : 0.9702 margin : 0.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [26:45<04:29, 269.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 2500/4082 Total loss: 0.0692  Current GPU memory usage: 2024 maxlen 7551.969211989692 \n",
      "idx 3 epoch 7 val set mean f1 : 0.6742 auc : 0.9653 margin : 0.0497\n",
      "idx 3 epoch 7 val set (fge) f1 : 0.6922 auc : 0.9702 margin : 0.0509\n",
      "Global step: 3000/4082 Total loss: 0.0694  Current GPU memory usage: 2024 maxlen 7552.0008526826705 \n",
      "idx 3 epoch 7 val set mean f1 : 0.6755 auc : 0.9663 margin : 0.0507\n",
      "idx 3 epoch 7 val set (fge) f1 : 0.6921 auc : 0.9703 margin : 0.0509\n",
      "Global step: 3500/4082 Total loss: 0.0725  Current GPU memory usage: 2024 maxlen 7551.957830560143 \n",
      "idx 3 epoch 7 val set mean f1 : 0.6746 auc : 0.9665 margin : 0.0516\n",
      "idx 3 epoch 7 val set (fge) f1 : 0.6931 auc : 0.9704 margin : 0.0510\n",
      "Global step: 4000/4082 Total loss: 0.0754  Current GPU memory usage: 2024 maxlen 7552.315646928475 \n",
      "idx 3 epoch 7 val set mean f1 : 0.6795 auc : 0.9665 margin : 0.0506\n",
      "idx 3 epoch 7 val set (fge) f1 : 0.6929 auc : 0.9705 margin : 0.0509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [33:40<00:00, 252.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold on validation set: 0.32 score 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 375806it [00:00, 1110195.60it/s]\n",
      "c:\\Users\\daksh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n",
      "generate lens: 261224it [00:00, 1011171.51it/s]\n",
      "generate lens: 1044898it [00:01, 1013476.23it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 500/16328 Total loss: 0.1387  Current GPU memory usage: 2024 maxlen 7556.674 \n",
      "Global step: 1000/16328 Total loss: 0.1147  Current GPU memory usage: 2024 maxlen 7555.84 \n",
      "Global step: 1500/16328 Total loss: 0.1097  Current GPU memory usage: 2024 maxlen 7554.315333333333 \n",
      "Global step: 2000/16328 Total loss: 0.1080  Current GPU memory usage: 2024 maxlen 7556.012 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [03:18<23:07, 198.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 0 val set f1 : 0.6697 auc : 0.9644 margin : 0.0601\n",
      "Global step: 2500/16328 Total loss: 0.1058  Current GPU memory usage: 2024 maxlen 7554.188 \n",
      "Global step: 3000/16328 Total loss: 0.1026  Current GPU memory usage: 2024 maxlen 7556.005666666667 \n",
      "Global step: 3500/16328 Total loss: 0.1015  Current GPU memory usage: 2024 maxlen 7556.542285714286 \n",
      "Global step: 4000/16328 Total loss: 0.0989  Current GPU memory usage: 2024 maxlen 7555.0055 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [06:36<19:50, 198.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 1 val set f1 : 0.6836 auc : 0.9674 margin : 0.0574\n",
      "Global step: 4500/16328 Total loss: 0.0971  Current GPU memory usage: 2024 maxlen 7556.230222222222 \n",
      "Global step: 5000/16328 Total loss: 0.0959  Current GPU memory usage: 2024 maxlen 7554.7964 \n",
      "Global step: 5500/16328 Total loss: 0.0961  Current GPU memory usage: 2024 maxlen 7558.110363636363 \n",
      "Global step: 6000/16328 Total loss: 0.0959  Current GPU memory usage: 2024 maxlen 7556.116333333333 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [09:55<16:32, 198.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 2 val set f1 : 0.6907 auc : 0.9694 margin : 0.0542\n",
      "Global step: 6500/16328 Total loss: 0.0924  Current GPU memory usage: 2024 maxlen 7556.4478461538465 \n",
      "Global step: 7000/16328 Total loss: 0.0900  Current GPU memory usage: 2024 maxlen 7556.024285714286 \n",
      "Global step: 7500/16328 Total loss: 0.0918  Current GPU memory usage: 2024 maxlen 7555.7996 \n",
      "Global step: 8000/16328 Total loss: 0.0922  Current GPU memory usage: 2024 maxlen 7556.50275 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [13:13<13:13, 198.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 3 val set f1 : 0.6886 auc : 0.9692 margin : 0.0554\n",
      "Global step: 8500/16328 Total loss: 0.0856  Current GPU memory usage: 2024 maxlen 7554.821764705883 \n",
      "Global step: 9000/16328 Total loss: 0.0869  Current GPU memory usage: 2024 maxlen 7554.077777777778 \n",
      "Global step: 9500/16328 Total loss: 0.0865  Current GPU memory usage: 2024 maxlen 7554.29852631579 \n",
      "Global step: 10000/16328 Total loss: 0.0862  Current GPU memory usage: 2024 maxlen 7553.5382 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [16:31<09:55, 198.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 4 val set f1 : 0.6929 auc : 0.9698 margin : 0.0517\n",
      "Global step: 10500/16328 Total loss: 0.0848  Current GPU memory usage: 2024 maxlen 7556.075047619048 \n",
      "Global step: 11000/16328 Total loss: 0.0819  Current GPU memory usage: 2024 maxlen 7555.729727272727 \n",
      "Global step: 11500/16328 Total loss: 0.0830  Current GPU memory usage: 2024 maxlen 7554.329565217391 \n",
      "Global step: 12000/16328 Total loss: 0.0842  Current GPU memory usage: 2024 maxlen 7556.31375 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [19:50<06:36, 198.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 5 val set f1 : 0.6912 auc : 0.9698 margin : 0.0548\n",
      "Global step: 500/4082 Total loss: 0.0822  Current GPU memory usage: 2024 maxlen 7555.285422877766 \n",
      "idx 4 epoch 6 val set mean f1 : 0.6880 auc : 0.9686 margin : 0.0500\n",
      "idx 4 epoch 6 val set (fge) f1 : 0.6880 auc : 0.9686 margin : 0.0500\n",
      "Global step: 1000/4082 Total loss: 0.0837  Current GPU memory usage: 2024 maxlen 7555.093311188283 \n",
      "idx 4 epoch 6 val set mean f1 : 0.6903 auc : 0.9688 margin : 0.0504\n",
      "idx 4 epoch 6 val set (fge) f1 : 0.6940 auc : 0.9702 margin : 0.0502\n",
      "Global step: 1500/4082 Total loss: 0.0858  Current GPU memory usage: 2024 maxlen 7555.098937872835 \n",
      "idx 4 epoch 6 val set mean f1 : 0.6848 auc : 0.9694 margin : 0.0534\n",
      "idx 4 epoch 6 val set (fge) f1 : 0.6968 auc : 0.9710 margin : 0.0513\n",
      "Global step: 2000/4082 Total loss: 0.0862  Current GPU memory usage: 2024 maxlen 7555.298399550751 \n",
      "idx 4 epoch 6 val set mean f1 : 0.6880 auc : 0.9697 margin : 0.0525\n",
      "idx 4 epoch 6 val set (fge) f1 : 0.6973 auc : 0.9716 margin : 0.0516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [26:47<04:30, 270.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 2500/4082 Total loss: 0.0691  Current GPU memory usage: 2024 maxlen 7555.40600840906 \n",
      "idx 4 epoch 7 val set mean f1 : 0.6837 auc : 0.9679 margin : 0.0488\n",
      "idx 4 epoch 7 val set (fge) f1 : 0.6988 auc : 0.9717 margin : 0.0510\n",
      "Global step: 3000/4082 Total loss: 0.0705  Current GPU memory usage: 2024 maxlen 7555.167519349337 \n",
      "idx 4 epoch 7 val set mean f1 : 0.6820 auc : 0.9675 margin : 0.0482\n",
      "idx 4 epoch 7 val set (fge) f1 : 0.6979 auc : 0.9718 margin : 0.0505\n",
      "Global step: 3500/4082 Total loss: 0.0736  Current GPU memory usage: 2024 maxlen 7555.5393750793855 \n",
      "idx 4 epoch 7 val set mean f1 : 0.6838 auc : 0.9675 margin : 0.0504\n",
      "idx 4 epoch 7 val set (fge) f1 : 0.6988 auc : 0.9719 margin : 0.0505\n",
      "Global step: 4000/4082 Total loss: 0.0733  Current GPU memory usage: 2024 maxlen 7555.439492798228 \n",
      "idx 4 epoch 7 val set mean f1 : 0.6820 auc : 0.9677 margin : 0.0507\n",
      "idx 4 epoch 7 val set (fge) f1 : 0.6994 auc : 0.9719 margin : 0.0505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [33:46<00:00, 253.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold on validation set: 0.36 score 0.7005\n",
      "avg of best threshold [0.29, 0.31, 0.34, 0.32, 0.36] macro-f1 best threshold 0.31 best score 0.6982523831139354\n",
      "coeff between predictions 0.9619804722195002\n"
     ]
    }
   ],
   "source": [
    "# seeding\n",
    "set_seed(233)\n",
    "epochs = 8\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "learning_rate_max_offset = 0.002\n",
    "fine_tuning_epochs = 2\n",
    "threshold = 0.31\n",
    "max_vocab_size = 120000\n",
    "embed_size = 300\n",
    "print_every_step = 500\n",
    "max_seq_len = 70\n",
    "share = True\n",
    "dropout = 0.1\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "train_df, test_df = load_data()\n",
    "# shuffling\n",
    "trn_idx = np.random.permutation(len(train_df))\n",
    "train_df = train_df.iloc[trn_idx].reset_index(drop=True)\n",
    "n_folds = 5\n",
    "n_repeats = 1\n",
    "args = {'epochs': epochs, 'batch_size': batch_size, 'learning_rate': learning_rate, 'threshold': threshold,\n",
    "        'max_vocab_size': max_vocab_size,\n",
    "        'embed_size': embed_size, 'print_every_step': print_every_step, 'dropout': dropout,\n",
    "        'learning_rate_max_offset': learning_rate_max_offset,\n",
    "        'fine_tuning_epochs': fine_tuning_epochs, 'max_seq_len': max_seq_len}\n",
    "predictions_te_all = np.zeros((len(test_df),))\n",
    "for _ in range(n_repeats):\n",
    "    if n_folds > 1:\n",
    "        _, predictions_te, _, threshold, coeffs = cv(train_df, test_df, n_folds=n_folds, share=share, **args)\n",
    "        print('coeff between predictions {}'.format(coeffs))\n",
    "    else:\n",
    "        predictions_te, _, _, _ = main(train_df, test_df, test_df, **args)\n",
    "    predictions_te_all += predictions_te / n_repeats\n",
    "sub.prediction = predictions_te_all > threshold\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = 'trained_model.pth'\n",
    "model_to_save = InsincereModel(\n",
    "    device=None,\n",
    "    hidden_dim=256,\n",
    "    hidden_dim_fc=16,\n",
    "    dropout=dropout,\n",
    "    embedding_matrixs=embedding_matrix,\n",
    "    vocab_size=len(vocab['token2id']),\n",
    "    embedding_dim=embed_size,\n",
    "    max_seq_len=max_seq_len\n",
    ")\n",
    "torch.save(model_to_save.state_dict(), model_path)\n",
    "print(\"Trained model saved to:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "def save_model(model, model_path):\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Load the model\n",
    "def load_model(model_architecture, model_path, device, hidden_dim, hidden_dim_fc, embedding_matrixs,\n",
    "               vocab_size, embedding_dim, max_seq_len):\n",
    "    model = model_architecture(device, hidden_dim=hidden_dim, hidden_dim_fc=hidden_dim_fc, dropout=dropout,\n",
    "                               embedding_matrixs=embedding_matrixs,\n",
    "                               vocab_size=vocab_size, embedding_dim=embedding_dim, max_seq_len=max_seq_len)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_49652\\3619896504.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Save the trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'trained_model.pth'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Trained model saved to:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_49652\\3172231390.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, model_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\daksh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model_path = 'trained_model.pth'\n",
    "save_model(sub, model_path)\n",
    "print(\"Trained model saved to:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "InsincereModel.__init__() missing 4 required positional arguments: 'device', 'hidden_dim', 'hidden_dim_fc', and 'embedding_matrixs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load the trained model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loaded_model \u001b[39m=\u001b[39m load_model(InsincereModel, model_path)\n",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_architecture, model_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(model_architecture, model_path):\n\u001b[1;32m----> 6\u001b[0m     model \u001b[39m=\u001b[39m model_architecture()\n\u001b[0;32m      7\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(model_path))\n\u001b[0;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "\u001b[1;31mTypeError\u001b[0m: InsincereModel.__init__() missing 4 required positional arguments: 'device', 'hidden_dim', 'hidden_dim_fc', and 'embedding_matrixs'"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "loaded_model = load_model(InsincereModel, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the loaded model\n",
    "predictions_te = eval_model(loaded_model, test_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Load the trained model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrained_model.pth\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 25\u001b[0m loaded_model \u001b[39m=\u001b[39m load_model(InsincereModel, model_path, \u001b[39mNone\u001b[39;00m, hidden_dim, hidden_dim_fc, embedding_matrixs, vocab_size, embed_size, max_seq_len)\n\u001b[0;32m     27\u001b[0m \u001b[39m# Save the loaded model (Optional)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m# model_path = 'loaded_model.pth'\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# save_model(loaded_model, model_path)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# print(\"Loaded model saved to:\", model_path)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[39m# Create the test dataset and DataLoader\u001b[39;00m\n\u001b[0;32m     33\u001b[0m test_dataset \u001b[39m=\u001b[39m TextDataset(test_df, vocab\u001b[39m=\u001b[39mshared_resources[\u001b[39m'\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m'\u001b[39m], max_seq_len\u001b[39m=\u001b[39mmax_seq_len)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hidden_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the InsincereModel class here\n",
    "# ...\n",
    "\n",
    "# Save the trained model\n",
    "def save_model(model, model_path):\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Load the model\n",
    "def load_model(model_architecture, model_path, device, hidden_dim, hidden_dim_fc, embedding_matrixs, vocab_size, embedding_dim, max_seq_len):\n",
    "    model = model_architecture(\n",
    "        device=device,\n",
    "        hidden_dim=hidden_dim,\n",
    "        hidden_dim_fc=hidden_dim_fc,\n",
    "        dropout=dropout,\n",
    "        embedding_matrixs=embedding_matrixs,\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "# Load the trained model\n",
    "model_path = 'trained_model.pth'\n",
    "loaded_model = load_model(InsincereModel, model_path, None, hidden_dim, hidden_dim_fc, embedding_matrixs, vocab_size, embed_size, max_seq_len)\n",
    "\n",
    "# Save the loaded model (Optional)\n",
    "# model_path = 'loaded_model.pth'\n",
    "# save_model(loaded_model, model_path)\n",
    "# print(\"Loaded model saved to:\", model_path)\n",
    "\n",
    "# Create the test dataset and DataLoader\n",
    "test_dataset = TextDataset(test_df, vocab=shared_resources['vocab'], max_seq_len=max_seq_len)\n",
    "tb = BucketSampler(test_dataset, test_dataset.get_keys(), batch_size=batch_size, shuffle_data=False)\n",
    "test_iter = DataLoader(dataset=test_dataset, batch_size=batch_size, sampler=tb, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Make predictions on the test set using the loaded model\n",
    "predictions_te = eval_model(loaded_model, test_iter, device)\n",
    "\n",
    "# Calculate accuracy of the model\n",
    "def calculate_accuracy(targets, predictions):\n",
    "    return (targets == (predictions > threshold)).mean()\n",
    "\n",
    "# Assuming you have the ground truth labels for the test set in a variable named 'test_labels'\n",
    "test_labels = test_df['target'].values\n",
    "\n",
    "accuracy = calculate_accuracy(test_labels, predictions_te)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
